# -*- coding: utf-8 -*-
"""breast cancer detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wEgcb10gKQEGMfqwd-s7P_UuXeQ9dyDk
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns

"""loading the dataset from kaggle and displaying first seven rows."""

from google.colab import files
uploaded=files.upload()
df=pd.read_csv('/content/data (1).csv')
df.head(7)

"""Printing the shape of the dataset.This gives us the output as (rows,cloumns)"""

df.shape

"""Finding the number of null values in the dataset."""

df.isnull().sum()

"""As seen earlier,this dataset has 32 columns with 569 null values,we would drop that whole column by the following code."""

df.dropna(axis=1)

df.shape

#to find count of the number of malignant(M) or Benign(B) cells
df['diagnosis'].value_counts()

#visualise the count
sns.countplot(df['diagnosis'],label="count")

df.dtypes

"""Importing scikitlearn and from that importing Label Encoder.
Label Encoder usually normalises labels.And it can also be used to transforn non-numerical values,which we have done here.
"""

import sklearn
from sklearn.preprocessing import LabelEncoder
labelencoder_Y=LabelEncoder()
labelencoder_Y.fit_transform(df.iloc[:,1].values)
df.iloc[:,1].values

"""Taking the same Label Encoder but this time instead of taking 'M' and 'B' we take the output as numeriacl value i.e. 1 and 0"""

import sklearn
from sklearn.preprocessing import LabelEncoder
labelencoder_Y=LabelEncoder()
labelencoder_Y.fit_transform(df.iloc[:,1].values)

import sklearn
from sklearn.preprocessing import LabelEncoder
labelencoder_Y=LabelEncoder()
df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)
df.iloc[:,1]

"""Now,visualising the data that we have.We are using seaborn library.
The following code displays three columns input and classifying them into Malignant and Benign cells.
"""

sns.pairplot(df.iloc[:,1:5],hue='diagnosis')

df.head()

"""Defining the correlation between this columns."""

df.iloc[:,1:12].corr()

#visualise the data
sns.heatmap(df.iloc[:,1:12].corr())

#split the dataset into independent X and dependent Y
X=df.iloc[:,2:31]
Y=df.iloc[:,1].values

#split dataset into training and testing 
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.fit_transform(X_test)

X_train

"""Now we will take 3 different algorithms and then the algorithm with the maximum accuracy would be used to train the data."""

def models(X_train,Y_train):
  
  #logistic Regression
  from sklearn.linear_model import LogisticRegression
  l=LogisticRegression(random_state=0)
  l.fit(X_train,Y_train)

  #Decision tree 
  from sklearn.tree import DecisionTreeClassifier
  tree=DecisionTreeClassifier(criterion='entropy',random_state=0)
  tree.fit(X_train,Y_train)

  #Random forest classifier
  from sklearn.ensemble import RandomForestClassifier
  forest=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)
  forest.fit(X_train,Y_train)

  #accuracy on training data
  print('[0]Logistic Regression Training Accuracy:',l.score(X_train,Y_train))
  print('[1]decision Tree Training Accuracy:',tree.score(X_train,Y_train))
  print('[2]Random Forest Training Accuracy:',forest.score(X_train,Y_train))

  return l,tree,forest

model=models(X_train,Y_train)

#test model accuracy on test data now
from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  print("Model",i)
  cm=confusion_matrix(Y_test,model[i].predict(X_test))

  TP=cm[0][0]
  TN=cm[1][0]
  FN=cm[1][0]
  FP=cm[0][1]

  print(cm)
  print("testing accuracy=",(TP+TN)/(TP+TN+FN+FP))
  print()

#print the prediction of random forest classifier model
pred=model[2].predict(X_test)
print(pred)

print(Y_test)